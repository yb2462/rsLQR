{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yb2462/rsLQR/blob/main/Copy_of_Cholesky.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Installing CUDA"
      ],
      "metadata": {
        "id": "g3L0y1jnLLyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sw3pBAsmdpZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#installing CUDA\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtthhJOiKtpz",
        "outputId": "84734370-e25a-4b85-debc-b2d5cb5d1e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU runtime\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2KCAXSCKxW3",
        "outputId": "d4f6ec11-87d5-4a7a-87db-90ee73a5fd2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  7 03:42:39 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# formatting helpers\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMLdwv-MKzpo",
        "outputId": "6327e822-f59c-4f38-b4a6-2471b20efde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-63_mqqty\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-63_mqqty\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4287 sha256=1d815742ff0a1c2350e11e29d5cc8ca1f9f5fd5b2f34996740511ebba1af3c19\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yajjvzzy/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O cpp_plugin.py https://gist.github.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd7b8QWwLiBJ",
        "outputId": "e7ee54bc-376c-4485-94d2-3dd27888384b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-07 03:42:46--  https://gist.github.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py\n",
            "Resolving gist.github.com (gist.github.com)... 192.30.255.112\n",
            "Connecting to gist.github.com (gist.github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://gist.githubusercontent.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py [following]\n",
            "--2023-06-07 03:42:46--  https://gist.githubusercontent.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2730 (2.7K) [text/plain]\n",
            "Saving to: ‘cpp_plugin.py’\n",
            "\n",
            "cpp_plugin.py       100%[===================>]   2.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-07 03:42:46 (33.7 MB/s) - ‘cpp_plugin.py’ saved [2730/2730]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cpp_plugin\n"
      ],
      "metadata": {
        "id": "fo8UTKx8LqJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cholesky Factorization - done and tested, should we pass n as well+should we put 0s?"
      ],
      "metadata": {
        "id": "bgX_8w-bLT6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cholesky Factorization** \n",
        "\n",
        "$A = L L^T$ \n",
        "where L is lower triangular matrix \n",
        "or \n",
        "\n",
        "$A = U^T U$ where U is an upper triangular matrix.\n",
        "\n",
        "We will implement the lower triangular factorization as it was implemented in the original rsLQR paper.\n",
        "\n",
        "*Check [link text](https://github.com/A2R-Lab/Block-Factorization/blob/main/SRI-23/choleskyDecomp_InPlace.cu)\n",
        "for column major order implementation +cgrps\n"
      ],
      "metadata": {
        "id": "HvwoNVeGLXze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -n run.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "\n",
        "\n",
        "/**\n",
        " * @brief Perform a Cholesky decomposition in place\n",
        " *\n",
        " * Performs a Cholesky decomposition on the square matrix @p s_A, storing the result in the\n",
        " * lower triangular portion of @p s_A.\n",
        " *\n",
        " * @param  s_A a square symmetric matrix \n",
        " * @return clap_kCholeskySuccess if successful, and clap_kCholeskyFail if not.\n",
        " */\n",
        "template <typename T> \n",
        "__device__ \n",
        "void cholDecomp_InPlace(T *s_A, int n) {\n",
        "    for (unsigned col = 0; col < n; col++) {\n",
        "        if (threadIdx.x == 0){\n",
        "            T sum = 0;\n",
        "            T val = s_A[n*col+col]; //entry Ljj\n",
        "            for(unsigned col_l = 0 ; col_l < col; col_l++) {\n",
        "                sum += pow(s_A[col*n+col_l],2);\n",
        "            }\n",
        "            s_A[col*n+col] = sqrt(val - sum);\n",
        "\n",
        "        }\n",
        "        __syncthreads(); //here we computed the diagonal entry of the column\n",
        "        // compute the rest of the column\n",
        "        for(unsigned row = threadIdx.x + col +1; row < n; row += blockDim.x){\n",
        "            T sum = 0;\n",
        "            T val = s_A[row*n+col];\n",
        "            for(unsigned k = 0; k < col; k++) {\n",
        "                sum += s_A[row*n+k]*s_A[col*n+k];\n",
        "            }\n",
        "            s_A[row*n+col] = (1.0/s_A[col*n+col])*(s_A[row*n+col]-sum);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T> //can put 9 as a template variable, or dynamically allocate array\n",
        "__global__ \n",
        "void choleskyDecomposition_InPlace_Kernel(T *d_A, int n) {\n",
        "    __shared__ T s_A[n*n];\n",
        "    // move RAM to shared\n",
        "    for(unsigned i = threadIdx.x; i < n*n; i += blockDim.x){s_A[i] = d_A[i]; }\n",
        "\n",
        "    // compute the decomposition\n",
        "    cholDecomp_InPlace<T>(s_A, n);\n",
        "\n",
        "    // move shared to RAM\n",
        "    for(unsigned i = threadIdx.x; i < n*n; i += blockDim.x){d_A[i] = s_A[i];}\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__host__\n",
        "int main() {\n",
        "    // Input matrix on the host\n",
        "    int n=3;\n",
        "    float A[n*n] = {6,15,55,15,55,225,55,225,979};\n",
        "\n",
        "\n",
        "    // Allocate memory on the GPU for the input and output matrices\n",
        "    float* d_A; cudaMalloc((void**)&d_A, n * n * sizeof(float));\n",
        "\n",
        "    // Copy the input matrix from the host to the GPU memory\n",
        "    cudaMemcpy(d_A, A, n * n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    //Launch the CUDA kernel with appropriate block and grid dimensions\n",
        "    int blockSize = 256;\n",
        "    int gridSize = 1;\n",
        "    choleskyDecomposition_InPlace_Kernel<float><<<gridSize, blockSize>>>(d_A, n);\n",
        "    cudaDeviceSynchronize();\n",
        "    // Copy the result back from the GPU memory to the host\n",
        "    cudaMemcpy(A, d_A, n * n * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    // Print the result\n",
        "    std::cout << \"Lower triangular matrix L:\" << std::endl;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            std::cout << A[i*n + j] << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free the allocated GPU memory\n",
        "    cudaFree(d_A);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "_ut3kcS0K8BY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bcbbde37-8b36-4a48-9ce9-a1447d94898d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/run.cu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc /content/src/run.cu -o run.exe\n",
        "!./run.exe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUB6SOSWJ50v",
        "outputId": "6c56068d-0ada-4ece-a8ed-0922177dc679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01m/content/src/run.cu(18)\u001b[0m: \u001b[01;35mwarning\u001b[0m #497-D: declaration of \"\u001b[01mn\u001b[0m\" hides template parameter\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/src/run.cu(45)\u001b[0m: \u001b[01;35mwarning\u001b[0m #497-D: declaration of \"\u001b[01mn\u001b[0m\" hides template parameter\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/src/run.cu(75)\u001b[0m: \u001b[01;31merror\u001b[0m: no instance of function template \u001b[01m\"choleskyDecomposition_InPlace_Kernel\"\u001b[0m matches the argument list\n",
            "            argument types are: (float *, int)\n",
            "\n",
            "1 error detected in the compilation of \"/content/src/run.cu\".\n",
            "Lower triangular matrix L:\n",
            "2.44949 15 55 \n",
            "6.12372 4.1833 225 \n",
            "22.4537 20.9165 6.1101 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fn1sF3OiG_uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful code from Brian"
      ],
      "metadata": {
        "id": "cnNYKPlVjwi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template <typename T, unsigned VEC_SIZE, unsigned SMALL_SWITCH = 3>\n",
        "__device__\n",
        "void reducePlus(T *data, cgrps::thread_block block){\n",
        "    unsigned size_left = VEC_SIZE;\n",
        "    // loop until only a few values left\n",
        "    while (size_left > SMALL_SWITCH){\n",
        "        // determine if odd_adjust needed and update size\n",
        "        bool odd_flag = size_left % 2;\n",
        "        size_left = (size_left - odd_flag)/2; \n",
        "        // reduce in half\n",
        "        for (unsigned ind = block.thread_rank(); ind < size_left; ind += block.num_threads()){\n",
        "            data[ind] += data[ind + size_left];\n",
        "        }\t\n",
        "        // add the odd size adjust if needed\n",
        "        if (block.thread_rank() == 0 && odd_flag){data[0] += data[2*size_left];}\n",
        "        // sync and repeat\n",
        "        block.sync();\n",
        "    }\n",
        "    // when we get really small sum up what is left\n",
        "    if (block.thread_rank() == 0){\n",
        "        for(unsigned ind = 1; ind < size_left; ind++){data[0] += data[ind];}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "cAkheISjkv3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/*******************************************************************************\n",
        "*                            matrix inversion                                  *\n",
        "*******************************************************************************/\n",
        "    \n",
        "\n",
        "// load identity in so memory is [A | I]\n",
        "template <typename T, unsigned DIM>\n",
        "__device__ __forceinline__\n",
        "void loadIdentity(T *A){\n",
        "    for (unsigned ind = GATO_THREAD_NUMBER; ind < DIM*DIM; ind += GATO_THREADS_PER_BLOCK){\n",
        "        unsigned r, c;\n",
        "        r = ind % DIM; \n",
        "        c = ind / DIM;\n",
        "        A[ind] = static_cast<T>(r == c);\n",
        "    }\n",
        "}\n",
        "\n",
        "// load identity in so memory is [V | I]\n",
        "template <typename T, unsigned DIMA, unsigned DIMB>\n",
        "__device__ __forceinline__\n",
        "void loadIdentity(T *A, T *B){\n",
        "    for (unsigned ind = GATO_THREAD_NUMBER; ind < DIMA*DIMA+DIMB*DIMB; ind += GATO_THREADS_PER_BLOCK){\n",
        "        unsigned r, c, indAdj; T *V;\n",
        "        if (ind < DIMA*DIMA){\n",
        "            indAdj = ind;\n",
        "            r = indAdj % DIMA; c = indAdj/DIMA; V = A;\n",
        "        }\n",
        "        else {\n",
        "            indAdj = ind - DIMA*DIMA;\n",
        "            r = indAdj % DIMB; c = indAdj/DIMB; V = B;\n",
        "        }\n",
        "        V[indAdj] = static_cast<T>(r == c);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// load identity in so memory is [V | I]\n",
        "template <typename T, unsigned DIMA, unsigned DIMB, unsigned DIMC>\n",
        "__device__ __forceinline__\n",
        "void loadIdentity(T *A, T *B, T *C){\n",
        "    for (unsigned ind = GATO_THREAD_NUMBER; ind < DIMA*DIMA+DIMB*DIMB+DIMC*DIMC; ind += GATO_THREADS_PER_BLOCK){\n",
        "        unsigned r, c, indAdj; T *V;\n",
        "        if (ind < DIMA*DIMA){\n",
        "            indAdj = ind;\n",
        "            r = indAdj % DIMA; c = indAdj/DIMA; V = A;\n",
        "        }\n",
        "        else if (ind < DIMA*DIMA+DIMB*DIMB){\n",
        "            indAdj = ind - DIMA*DIMA;\n",
        "            r = indAdj % DIMB; c = indAdj/DIMB; V = B;\n",
        "        }\n",
        "        else{\n",
        "            indAdj = ind - DIMA*DIMA - DIMB*DIMB;\n",
        "            r = indAdj % DIMC; c = indAdj/DIMC; V = C;\n",
        "        }\n",
        "        V[indAdj] = static_cast<T>(r == c);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "template <typename T, unsigned DIM>\n",
        "__device__\n",
        "void invertMatrix(T *A, T *s_temp){ \n",
        "// we are going to guassian elimination walking down the matrix (assuming no leading 0s)\n",
        "// we therefore use the columns in order as the pivot column for each pivot we need to rescale \n",
        "// that row so that the pivot value (pv) is 1 THEN for all other row values (orv) we need to add a multiple \n",
        "// of the NEW pivot row value (prv) such that we transorm the other row pivot column value (orpcv) to 0\n",
        "// pr *= 1/pv   orv -= orpcv*prv == orv -= orpcv*1/pv*prvOld\n",
        "    for (unsigned pivRC = 0; pivRC < DIM; pivRC++){\n",
        "        unsigned pivColOffset = pivRC*DIM;\n",
        "        // save the pivot and pivot column and row\n",
        "        T pvInv = static_cast<T>(1)/A[pivRC + pivColOffset];\n",
        "        for (unsigned ind = GATO_THREAD_NUMBER; ind < 2*DIM+1; ind++){\n",
        "            unsigned AInd;\n",
        "            if (ind < DIM){AInd = ind + pivColOffset;}\n",
        "            else{AInd = pivRC + pivColOffset + (ind-DIM)*DIM;}\n",
        "            s_temp[ind] = A[AInd];\n",
        "        }\n",
        "        __syncthreads(); //----------------------\n",
        "        // make the pivot update\n",
        "        for (unsigned ind = GATO_THREAD_NUMBER; ind < DIM*(DIM+1); ind += GATO_THREADS_PER_BLOCK){\n",
        "            unsigned row = ind % DIM; unsigned col = ind / DIM; unsigned colOffset = ind - row;\n",
        "            // s_temp = orpcvs|prvOld\n",
        "            if (row == pivRC){A[row + pivColOffset + colOffset] *= pvInv;}\n",
        "            else{A[row + pivColOffset + colOffset] -= s_temp[row]*pvInv*s_temp[DIM+col];}\n",
        "        }\n",
        "    __syncthreads(); //----------------------\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "template <typename T, unsigned DIMA, unsigned DIMB, unsigned MAX_DIM>\n",
        "__device__\n",
        "void invertMatrix(T *A, T *B, T *s_temp){\n",
        "\n",
        "    // now we are going to guassian elimination walking down the matrix (assuming no leading 0s)\n",
        "    // we therefore use the columns in order as the pivot column for each pivot we need to rescale \n",
        "    // that row so that the pivot value (pv) is 1 THEN for all other row values (orv) we need to add a multiple \n",
        "    // of the NEW pivot row value (prv) such that we transorm the other row pivot column value (orpcv) to 0\n",
        "    // pr *= 1/pv   orv -= orpcv*prv == orv -= orpcv*1/pv*prvOld\n",
        "T *s_memA = s_temp; T *s_memB = &s_memA[2*DIMA+1];\n",
        "    for (unsigned pivRC = 0; pivRC < MAX_DIM; pivRC++){\n",
        "        bool AActive = pivRC < DIMA; bool BActive = pivRC < DIMB;\n",
        "        unsigned pivColOffsetA = pivRC*DIMA; unsigned pivColOffsetB = pivRC*DIMB;\n",
        "        // save the pivot column and row\n",
        "        for (unsigned ind = GATO_THREAD_NUMBER; ind < MAX_DIM; ind++){\n",
        "            if (AActive && ind < DIMA){s_memA[ind] = A[ind + pivColOffsetA];}\n",
        "            if (BActive && ind < DIMB){s_memB[ind] = B[ind + pivColOffsetB];}\n",
        "        }\n",
        "        for (unsigned ind = GATO_THREAD_NUMBER; ind < MAX_DIM+1; ind++){\n",
        "            if (AActive && ind < DIMA+1){s_memA[ind + DIMA] = A[ind*DIMA + pivRC + pivColOffsetA];}\n",
        "            if (BActive && ind < DIMB+1){s_memB[ind + DIMB] = B[ind*DIMB + pivRC + pivColOffsetB];}\n",
        "        }\n",
        "        __syncthreads(); //----------------------\n",
        "        // make the pivot update with s_mem = [colA,rowA,colB,rowB,colC,rowC]\n",
        "        for (unsigned ind = GATO_THREAD_NUMBER; ind < MAX_DIM*(MAX_DIM+1); ind += GATO_THREADS_PER_BLOCK){\n",
        "            if (AActive && ind < DIMA*(DIMA+1)){\n",
        "                unsigned row = ind % DIMA; unsigned col = ind / DIMA;\n",
        "                if (row == pivRC){A[pivColOffsetA + ind] /= s_memA[pivRC];}\n",
        "                else{A[pivColOffsetA + ind] -= s_memA[row]/s_memA[pivRC]*s_memA[DIMA+col];}\n",
        "            }\n",
        "            if (BActive && ind < DIMB*(DIMB+1)){\n",
        "                unsigned row = ind % DIMB; unsigned col = ind / DIMB; \n",
        "                if (row == pivRC){B[pivColOffsetB + ind] /= s_memB[pivRC];}\n",
        "                else{B[pivColOffsetB + ind] -= s_memB[row]/s_memB[pivRC]*s_memB[DIMB+col];}\n",
        "            }\n",
        "        }\n",
        "        __syncthreads(); //----------------------\n",
        "    }\n",
        "}\n",
        "\n",
        "// invert A,B,C assume memory for all is [V | VInv] where both are DIMxDIM and continguous\n",
        "// relies on s_temp of size [2*DIMA + 2*DIMB + 2*DIMC + 3]\n",
        "template <typename T, unsigned DIMA, unsigned DIMB, unsigned DIMC, unsigned MAX_DIM>\n",
        "__device__\n",
        "void invertMatrix(T *A, T *B, T *C, T *s_temp){\n",
        "\n",
        "    // now we are going to guassian elimination walking down the matrix (assuming no leading 0s)\n",
        "    // we therefore use the columns in order as the pivot column for each pivot we need to rescale \n",
        "    // that row so that the pivot value (pv) is 1 THEN for all other row values (orv) we need to add a multiple \n",
        "    // of the NEW pivot row value (prv) such that we transorm the other row pivot column value (orpcv) to 0\n",
        "    // pr *= 1/pv   orv -= orpcv*prv == orv -= orpcv*1/pv*prvOld\n",
        "    T *s_memA = s_temp; T *s_memB = &s_memA[2*DIMA+1]; T *s_memC = &s_memB[2*DIMB+1];\n",
        "    for (unsigned pivRC = 0; pivRC < MAX_DIM; pivRC++){\n",
        "        bool AActive = pivRC < DIMA; bool BActive = pivRC < DIMB; bool CActive = pivRC < DIMC;\n",
        "        unsigned pivColOffsetA = pivRC*DIMA; unsigned pivColOffsetB = pivRC*DIMB; unsigned pivColOffsetC = pivRC*DIMC;\n",
        "        // save the pivot column and row\n",
        "        for (unsigned ind = GATO_THREAD_NUMBER; ind < MAX_DIM; ind++){\n",
        "            if (AActive && ind < DIMA){s_memA[ind] = A[ind + pivColOffsetA];}\n",
        "            if (BActive && ind < DIMB){s_memB[ind] = B[ind + pivColOffsetB];}\n",
        "            if (CActive && ind < DIMC){s_memC[ind] = C[ind + pivColOffsetC];}\n",
        "        }\n",
        "        for (unsigned ind = GATO_THREAD_NUMBER; ind < MAX_DIM+1; ind++){\n",
        "            if (AActive && ind < DIMA+1){s_memA[ind + DIMA] = A[ind*DIMA + pivRC + pivColOffsetA];}\n",
        "            if (BActive && ind < DIMB+1){s_memB[ind + DIMB] = B[ind*DIMB + pivRC + pivColOffsetB];}\n",
        "            if (CActive && ind < DIMC+1){s_memC[ind + DIMC] = C[ind*DIMC + pivRC + pivColOffsetC];}\n",
        "        }\n",
        "        __syncthreads(); //----------------------\n",
        "        // make the pivot update with s_mem = [colA,rowA,colB,rowB,colC,rowC]\n",
        "        for (unsigned ind = GATO_THREAD_NUMBER; ind < MAX_DIM*(MAX_DIM+1); ind += GATO_THREADS_PER_BLOCK){\n",
        "            if (AActive && ind < DIMA*(DIMA+1)){\n",
        "                unsigned row = ind % DIMA; unsigned col = ind / DIMA;\n",
        "                if (row == pivRC){A[pivColOffsetA + ind] /= s_memA[pivRC];}\n",
        "                else{A[pivColOffsetA + ind] -= s_memA[row]/s_memA[pivRC]*s_memA[DIMA+col];}\n",
        "            }\n",
        "            if (BActive && ind < DIMB*(DIMB+1)){\n",
        "                unsigned row = ind % DIMB; unsigned col = ind / DIMB; \n",
        "                if (row == pivRC){B[pivColOffsetB + ind] /= s_memB[pivRC];}\n",
        "                else{B[pivColOffsetB + ind] -= s_memB[row]/s_memB[pivRC]*s_memB[DIMB+col];}\n",
        "            }\n",
        "            if (CActive && ind < DIMC*(DIMC+1)){\n",
        "                unsigned row = ind % DIMC; unsigned col = ind / DIMC;\n",
        "                if (row == pivRC){C[pivColOffsetC + ind] /= s_memC[pivRC];}\n",
        "                else{C[pivColOffsetC + ind] -= s_memC[row]/s_memC[pivRC]*s_memC[DIMC+col];}\n",
        "            }\n",
        "        }\n",
        "        __syncthreads(); //----------------------\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "TiM0TTe8j5sq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "129c0e78-300a-4237-b2bc-1aa65c7a7b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a1535602e31e>\"\u001b[0;36m, line \u001b[0;32m63\u001b[0m\n\u001b[0;31m    // we are going to guassian elimination walking down the matrix (assuming no leading 0s)\u001b[0m\n\u001b[0m                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/**\n",
        "     * Compute the dot product between two vectors\n",
        "     *\n",
        "     * Notes:\n",
        "     *   Assumes computed by a single thread\n",
        "     *\n",
        "     * @param vec1 is the first vector of length N with stride S1\n",
        "     * @param vec2 is the second vector of length N with stride S2\n",
        "     * @return the resulting final value\n",
        "     */\n",
        "    template <typename T, int N, int S1, int S2>\n",
        "    __device__\n",
        "    T dot_prod(const T *vec1, const T *vec2) {\n",
        "        T result = 0;\n",
        "        for(int i = 0; i < N; i++) {\n",
        "            result += vec1[i*S1] * vec2[i*S2];\n",
        "        }\n",
        "        return result;\n",
        "    }\n",
        "\n",
        "    /**\n",
        "     * Compute the dot product between two vectors\n",
        "     *\n",
        "     * Notes:\n",
        "     *   Assumes computed by a single thread\n",
        "     *\n",
        "     * @param vec1 is the first vector of length N with stride S1\n",
        "     * @param vec2 is the second vector of length N with stride S2\n",
        "     * @return the resulting final value\n",
        "     */\n",
        "    template <typename T, int N, int S1, int S2>\n",
        "    __device__\n",
        "    T dot_prod(T *vec1, const T *vec2) {\n",
        "        T result = 0;\n",
        "        for(int i = 0; i < N; i++) {\n",
        "            result += vec1[i*S1] * vec2[i*S2];\n",
        "        }\n",
        "        return result;\n",
        "    }\n",
        "\n",
        "    /**\n",
        "     * Compute the dot product between two vectors\n",
        "     *\n",
        "     * Notes:\n",
        "     *   Assumes computed by a single thread\n",
        "     *\n",
        "     * @param vec1 is the first vector of length N with stride S1\n",
        "     * @param vec2 is the second vector of length N with stride S2\n",
        "     * @return the resulting final value\n",
        "     */\n",
        "    template <typename T, int N, int S1, int S2>\n",
        "    __device__\n",
        "    T dot_prod(const T *vec1, T *vec2) {\n",
        "        T result = 0;\n",
        "        for(int i = 0; i < N; i++) {\n",
        "            result += vec1[i*S1] * vec2[i*S2];\n",
        "        }\n",
        "        return result;\n",
        "    }\n",
        "\n",
        "    /**\n",
        "     * Compute the dot product between two vectors\n",
        "     *\n",
        "     * Notes:\n",
        "     *   Assumes computed by a single thread\n",
        "     *\n",
        "     * @param vec1 is the first vector of length N with stride S1\n",
        "     * @param vec2 is the second vector of length N with stride S2\n",
        "     * @return the resulting final value\n",
        "     */\n",
        "    template <typename T, int N, int S1, int S2>\n",
        "    __device__\n",
        "    T dot_prod(T *vec1, T *vec2) {\n",
        "        T result = 0;\n",
        "        for(int i = 0; i < N; i++) {\n",
        "            result += vec1[i*S1] * vec2[i*S2];\n",
        "        }\n",
        "        return result;\n",
        "    }"
      ],
      "metadata": {
        "id": "xLwZRUZlj71B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LowerTriBackSub - Tested, Works\n",
        "Now from $Ax=b$  we have $LL^Tx=b$\n",
        " $$[\n",
        " L y = b\n",
        " ]$$\n",
        " for a lower-triangular matrix $L$, or\n",
        " $$[\n",
        "  L^T x =y\n",
        " ]$$\n"
      ],
      "metadata": {
        "id": "K0YLJ4qqLejc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -n lowerBackSub.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "\n",
        "//Solves linear system of equations for a lower-trianguler matrix\n",
        "\n",
        "\n",
        "template <typename T> \n",
        "__device__ \n",
        "void lowerBackSub_InPlace(T *s_A, T *s_b, bool istransposed, int n, int m) {\n",
        "    for (unsigned col = 0; col < n; col++) {\n",
        "        if (threadIdx.x == 0){\n",
        "            for(unsigned col_l = 0 ; col_l < col; col_l++) {\n",
        "                if (istransposed) {\n",
        "                    s_b[col] -= s_A[col_l*n+col]; \n",
        "                }\n",
        "                else {\n",
        "                    s_b[col] -=s_A[col*n+col_l];\n",
        "                }\n",
        "            }\n",
        "            s_b[col] /= s_A[col*n+col];            \n",
        "        }\n",
        "        __syncthreads(); //here we computed the b_col\n",
        "\n",
        "        // multiply matrix by b_col\n",
        "        for(unsigned row = threadIdx.x + col +1; row < n; row += blockDim.x){\n",
        "           if (istransposed) {\n",
        "              s_A[col*n+row] = s_A[col*n+row]*s_b[col];\n",
        "           } else {\n",
        "              s_A[row*n+col] = s_A[row*n+col]*s_b[col];\n",
        "           }\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "}\n",
        "__device__\n",
        "void SolveLeaf(T *s_A) {\n",
        "\n",
        "     if(blockIdx.x == 0){\n",
        "        lowerbacksub()\n",
        "        \n",
        "     }\n",
        "\n",
        "     for (i =  blockIdx.x, i < n; i += gridDim.x){\n",
        "        ddevice)_funct(i)\n",
        "     }\n",
        "\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "__global__ \n",
        "void lowerBackSub_InPlace_Kernel(T *d_A, T *d_b, bool istransposed, int n,int m) {\n",
        "    __shared__ T s_A[9];\n",
        "    __shared__ T s_b[3];\n",
        "\n",
        "    extern __shared__ T s_temp[];\n",
        "    T *s_A = s_temp;\n",
        "    T *s_B = s_A + \n",
        "\n",
        "    // move RAM to shared\n",
        "    for(unsigned i = threadIdx.x; i < n*n; i += blockDim.x)  {\n",
        "        s_A[i] = d_A[i]; s_b[i] = d_b[i];}\n",
        "\n",
        "\n",
        "    // compute the decomposition\n",
        "    lowerBackSub_InPlace<T>(s_A, s_b, istransposed, n);\n",
        "\n",
        "    // move shared to RAM\n",
        "    for(unsigned i = threadIdx.x; i < n*n; i += blockDim.x) {\n",
        "        d_A[i] = s_A[i]; d_b[i] = s_b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__host__\n",
        "int main() {\n",
        "    // Input matrix on the host\n",
        "    int n=3;\n",
        "    float A[n*n] = {1,1,1,0,1,1,0,0,1};\n",
        "    float b[n] = {10,7,6};\n",
        "    bool istransposed = true;\n",
        "\n",
        "\n",
        "    // Allocate memory on the GPU for the input and output matrices\n",
        "    float* d_A; cudaMalloc((void**)&d_A, n * n * sizeof(float));\n",
        "    float* d_b; cudaMalloc((void**)&d_b, n*sizeof(int));\n",
        "\n",
        "    // Copy the input matrix from the host to the GPU memory\n",
        "    cudaMemcpy(d_A, A, n * n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, n*sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    //Launch the CUDA kernel with appropriate block and grid dimensions\n",
        "    int blockSize = 256;\n",
        "    int gridSize = 1;\n",
        "    lowerBackSub_InPlace_Kernel<float><<<gridSize, blockSize, n>>>(d_A,d_b, istransposed);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy the result back from the GPU memory to the host\n",
        "    cudaMemcpy(A, d_A, n * n * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(b, d_b, n *sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    \n",
        "    // Print the result\n",
        "    std::cout << \"solution vector b:\" << std::endl;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        std::cout << b[i] << \" \";\n",
        "      \n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free the allocated GPU memory\n",
        "    cudaFree(d_A); cudaFree(d_b);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "cgSvApdwMgA5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45646c87-189e-4ac4-c527-b1b11b92bd8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/lowerBackSub.cu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "modified version that solves for $AX=B$"
      ],
      "metadata": {
        "id": "hb0uVp9PqdxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -n lowerBackSub.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "\n",
        "//Solves linear system of equations for a lower-trianguler matrix\n",
        "\n",
        "\n",
        "template <typename T> \n",
        "__device__ \n",
        "\n",
        "// A - nxn; X - nxm; B nxm;  \n",
        "void lowerBackSub_InPlace(T *s_A, T *s_B, bool istransposed, int n, int m) {\n",
        "    unsigned k = threadIdx.x;\n",
        "    if (k < m) {\n",
        "        if (istransposed) {\n",
        "            for (int col = n-1; col >= 0; col--) {\n",
        "                for(int col_l = n-1; col_l > col; col_l--) {\n",
        "                    // s_B[k*n + col] -= s_A[col_l*n+col] * s_B[k*n + col_l]; by right it should be this but because the matrix we put in the the lower\n",
        "                     s_B[k*n + col] -= s_A[col_l*n+col] * s_B[k*n + col_l];\n",
        "                }\n",
        "                // s_B[k*n + col] /= s_A[col*n+col];\n",
        "            }\n",
        "        } else {\n",
        "            for (unsigned col = 0; col < n; col++) {\n",
        "                for(unsigned col_l = 0 ; col_l < col; col_l++) {\n",
        "                    s_B[k*n + col] -= s_A[col_l*n+col] * s_B[k*n + col_l];\n",
        "                }\n",
        "                s_B[k*n + col] /= s_A[col*n+col];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    __syncthreads(); // here we computed the b_col\n",
        "}\n",
        "\n",
        "\n",
        "template <typename T>\n",
        "__global__ \n",
        "void lowerBackSub_InPlace_Kernel(T *d_A, T *d_B, bool istransposed, int n, int m) {\n",
        "    __shared__ T s_A[9]; // maybe this is okay to hardcode since the matrices put in are definitely this size? not sure\n",
        "    __shared__ T s_B[6]; // what should the dimensions of this be? maybe make it 3x2 for the timebeing?\n",
        "\n",
        "    // move RAM to shared\n",
        "    // does this mean that all of the threads are doing this?\n",
        "    // fast way of moving all this into RAM - each thread responsible for moving the data\n",
        "    for(unsigned i = threadIdx.x; i < n*n; i += blockDim.x)  {\n",
        "        s_A[i] = d_A[i]; s_B[i] = d_B[i];}\n",
        "\n",
        "\n",
        "    // compute the decomposition\n",
        "    lowerBackSub_InPlace<T>(s_A, s_B, istransposed, n, m);\n",
        "\n",
        "    // move shared to RAM\n",
        "    for(unsigned i = threadIdx.x; i < n*n; i += blockDim.x) {\n",
        "        d_A[i] = s_A[i];\n",
        "    }\n",
        "    for(unsigned i = threadIdx.x; i < n*m; i += blockDim.x) {\n",
        "        d_B[i] = s_B[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__host__\n",
        "int main() {\n",
        "    // Input matrix on the host\n",
        "    int n=3;\n",
        "    int m=2;\n",
        "    float A[n*n] = {2,0,0,1,1,0,3,3,3};\n",
        "    float B[n*m] = {10,7,6,10,7,6};\n",
        "    bool istransposed = true;\n",
        "\n",
        "\n",
        "    // Allocate memory on the GPU for the input and output matrices\n",
        "    float* d_A; cudaMalloc((void**)&d_A, n * n * sizeof(float));\n",
        "    float* d_B; cudaMalloc((void**)&d_B, n * m * sizeof(int));\n",
        "\n",
        "    // Copy the input matrix from the host to the GPU memory\n",
        "    cudaMemcpy(d_A, A, n * n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, B, n * m * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    //Launch the CUDA kernel with appropriate block and grid dimensions\n",
        "    int blockSize = 256;\n",
        "    int gridSize = 1;\n",
        "    lowerBackSub_InPlace_Kernel<float><<<gridSize, blockSize>>>(d_A,d_B, istransposed, n, m);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy the result back from the GPU memory to the host\n",
        "    cudaMemcpy(A, d_A, n * n * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(B, d_B, n * m * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    \n",
        "    // Print the result\n",
        "    std::cout << \"solution matrix X:\" << std::endl;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < m; j++) {\n",
        "            std::cout << B[j*n+i] << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free the allocated GPU memory\n",
        "    cudaFree(d_A); cudaFree(d_B);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "Dz5pAABbqZYW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2576f99f-30a0-409a-a09b-75d7b6141db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/lowerBackSub.cu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cudaDevicesybchronize - synch the kernel\n"
      ],
      "metadata": {
        "id": "5vMvnZ0mSXnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc /content/src/lowerBackSub.cu -o lowerBackSub.exe\n",
        "!./lowerBackSub.exe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkLK51qp5WSY",
        "outputId": "d3b6f8ed-ffd1-403f-8b97-b1f94247eef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "solution matrix X:\n",
            "5 5 \n",
            "7 7 \n",
            "2 2 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cholesky Solve - Not tested\n",
        "\n",
        "Solves a linear system of equation with a precomputed Cholesky decomposition.\n",
        "\n",
        " * @param[in]    A A square matrix whose Cholesky decomposition is stored in the lower triangular portion of the matrix\n",
        " * @param[inout] b The right-hand-side vector. Stores the solution upon completion of the function.\n",
        " * @return 0 if successful"
      ],
      "metadata": {
        "id": "d1eIMvLDHqJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -n CholeskySolve.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "\n",
        "//Solves linear system of equations with Cholesky\n",
        "template <typename T> \n",
        "__device__ \n",
        "void cholSolve_InPlace(T *s_A, T *s_b, bool istransposed, int n, int m) {\n",
        "    lowerBackSub_InPlace<T>(s_A, s_b, 0, n, m);\n",
        "    lowerBackSub_InPlace<T>(s_A, s_b, 1, n, m);\n",
        "}\n",
        "\n",
        "\n",
        "template <typename T>\n",
        "__global__ \n",
        "void CholSolve_InPlace_Kernel(T *d_A, T *d_b, int n) {\n",
        "    __shared__ T s_A[9];\n",
        "    __shared__ T s_b[3];\n",
        "\n",
        "    // move RAM to shared\n",
        "    for(unsigned i = threadIdx.x; i < n*n; i += blockDim.x)  {\n",
        "        s_A[i] = d_A[i]; s_b[i] = d_b[i];}\n",
        "\n",
        "\n",
        "    CholSolve_InPlace<T>(s_A, s_b, false, n);\n",
        "\n",
        "    // move shared to RAM\n",
        "    for(unsigned i = threadIdx.x; i < n*n; i += blockDim.x) {\n",
        "        d_A[i] = s_A[i]; d_b[i] = s_b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__host__\n",
        "int main() {\n",
        "    // Input matrix on the host\n",
        "    int n=3;\n",
        "    float A[n*n] = {2,0,1,1,2,3,1,0,3};\n",
        "    float b[n] = {6,7,6};\n",
        "    bool istransposed = false;\n",
        "\n",
        "\n",
        "    // Allocate memory on the GPU for the input and output matrices\n",
        "    float* d_A; cudaMalloc((void**)&d_A, n * n * sizeof(float));\n",
        "    float* d_b; cudaMalloc((void**)&d_b, n*sizeof(int));\n",
        "\n",
        "    // Copy the input matrix from the host to the GPU memory\n",
        "    cudaMemcpy(d_A, A, n * n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, n*sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    //Launch the CUDA kernel with appropriate block and grid dimensions\n",
        "    int blockSize = 256;\n",
        "    int gridSize = 1;\n",
        "    CholSolve_InPlace_Kernel<float><<<gridSize, blockSize>>>(d_A,d_b, istransposed, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy the result back from the GPU memory to the host\n",
        "    cudaMemcpy(A, d_A, n * n * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(b, d_b, n *sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    \n",
        "    // Print the result\n",
        "    std::cout << \"solution vector b:\" << std::endl;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        std::cout << b[i] << \" \";\n",
        "      \n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free the allocated GPU memory\n",
        "    cudaFree(d_A); cudaFree(d_b);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "dJRCLFKGIIDN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3ae5fb0-6298-4a90-eb87-4da2dccc3053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/CholeskySolve.cu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#GLASS\n"
      ],
      "metadata": {
        "id": "R-kXDK5Beqij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "//L1 - vector by vector/scalar\n",
        "#include axpy.cuh //multiply vector x*alp+y\n",
        "#include copy.cuh // copy vector x to y\n",
        "#include dot.cuh // dot product between x and y vectors\n",
        "#include ident.cuh // load identity next to matrix\n",
        "#inclide scal.cuh // multiply vector by scalar\n",
        "#include swap.cuh //swap x and y vectors\n",
        "//L2 - matrix by vector\n",
        "#include gemv.cuh //just matrix by vector?\n",
        "#include gemm.cuh //NOT CHECKED\n",
        "#include inv.cuh //invert matrix"
      ],
      "metadata": {
        "id": "op51dktWg30f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Memory Explanation**"
      ],
      "metadata": {
        "id": "dVTInrpIuTvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "linear index = index+ nhorizon*level\n",
        "\n",
        "**C factor** is usually used as solver->data \\\\\n",
        ">**C.state = A matrix**, nstates^2 \\\\\n",
        "to access C.state use \n",
        "A_B[k * dyn_step] where k is a time step or linear index\n",
        "\n",
        ">**C.input = B matrix** , nstates*ninputs \\\\\n",
        "to access C.input use A_B[k * dyn_step+(nstates*nstates] where k is a linear index\\\\\n",
        " \n",
        " >C.lambda - UNUSED PORTION OF THE NDFACTOR\n",
        "\n",
        "\n",
        " **zFactor is usually used as solver -> soln**\\\\\n",
        " >**z.state = q vector** of nstates \\\\\n",
        " to access z.state use q_r[k * (nstates+ninputs)] \\\\\n",
        "\n",
        " >**z.input = r** vector of size ninputs \\\\\n",
        " to access z.input use q_r[k * (nstates+ninputs)+nstates]\n",
        "\n",
        " >**z.lambda = $x_0$+d , total vector of size nstates** \\\\\n",
        " to access z.lambda use d[k *  nstates] \\\\\n",
        "\n",
        "Q and R matrices are stored in diagonals array in Brian's code and in *Q_R in ours.\n",
        " >**diagonals[2k] stands for Q matrix**, nstates*nstates \\\\\n",
        " to access use Q_R[k * cost_step]\n",
        "\n",
        " >**diagonals[2k+1] stands for R matrix**, ninputs*ninputs \\\\\n",
        " to access use Q_R[k*coststep+(nstates*nstates)]\n",
        "\n",
        "\n",
        "**Need to come up with array structure for F**.\n",
        ">F.state = matrix of nstates^2 \\\\\n",
        "F.input = matrix of ninput*nstates \\\\\n",
        "F.lambda = matrix of nstates^2 \\\\\n",
        "\n",
        "DURING/AFTER SOLVE LEAF\n",
        ">F.lambda stores results of C.state*-1\n",
        ">F.state stores 0.0 for k=0, and then stores MatrixCholeskySolveWithInfo(Q, &C->state,\n",
        "                                  Qchol)  \n",
        "\n",
        ">F.input stores results of cholSolve(R,&C->input),at the last step initializes it with 0.0\n",
        "\n",
        "                  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QGO_TuK6Roe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "/*Our implementation of memory layout, all matrices should be entered \n",
        "column major order*/\n",
        "\n",
        " if we want to save space to have Q_R as vectors:\n",
        "//int cost_step = nstates+ninputs; //size of one timestep in array\n",
        "\n",
        "//or if needed to have Q_R as matrices:\n",
        "int cost_step = nstates*nstates+ninputs*ninputs; //size of one timestep in array\n",
        "double* Q_R = (double*)malloc(nhorizon*cost_step*sizeof(double)); //-ninputs\n",
        "double* q_r = (double*)malloc((nstates+ninputs)*nhorizon*sizeof(double)); //-ninputs\n",
        "\n",
        "int dyn_step = nstates*nstates+ninputs*ninputs; //size of one timestep in array\n",
        "double* A_B = (double*)malloc(nhorizon*dyn_step*sizeof(double));\n",
        "//A,B should be entered as transposed matrices to A_B\n",
        "double* c = (double*)malloc(nhorizon*sizeof(double));\n",
        "double* d = (double*)malloc(nstates*nhorizon*sizeof(double));\n",
        "\n",
        "\n",
        "//For factorized infor prob want\n",
        "double* F_Qsolv;\n",
        "double* F_Rsolv;\n",
        "double* F_lambda; //??\n",
        "\n",
        "\n",
        "\n",
        "//to go from Brian's tree to linear index\n",
        "lin_index = index+ nhorizon*level;\n",
        "\n",
        "//Figure out the size of the array for F.state+F.input, not sure if we need it\n",
        "double* Fst_inp = (double*)malloc(nhorizon*)"
      ],
      "metadata": {
        "id": "LeY1pe06xYAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//From Brian's paper\n",
        "int cost_size = 2*nstates + 2*ninputs+1; // Q,R,q,r,c\n",
        "\n",
        "int dynamics_size = nstates*nstates +nstates*ninputs +nstates; //A.B.d\n",
        "//int total_size = cost_size+dynamics_size; //in case we'll want have one array later one\n",
        "\n",
        "double* data_cost = (double*)malloc(total_size*sizeof(double));\n",
        "double* Q = data;\n",
        "double* R = data+nstates; //skips over nstates (which is Q)\n",
        "double* q = data + nstates + ninputs; //skips over Q,R\n",
        "double* r = data + 2*nstates +ninputs; //r is size ninput\n",
        "double* c = data+ 2 * nstates +2 * ninputs; \n",
        "double* A = data+ cost_size ; \n",
        "double* B = data+cost_size+ nstates*nstates;\n",
        "double * d = data+cost_size+nstates*nstates+nstates*ninputs;"
      ],
      "metadata": {
        "id": "PQhreTeuuSh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Solve Leaf**"
      ],
      "metadata": {
        "id": "VUBkJXWbFPzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "//GetNdFactor(nddata,index,level, &factor)\n",
        "#include \"./GLASS/GLASS.cuh\"\n",
        "#include <cstdint>\n",
        "#include <cooperative_groups.h>\n",
        "namespace cgrps = cooperative_groups;\n",
        "\n",
        "\n",
        "__device__\n",
        "void solveLeaf(int index, int nstates, int ninputs, int nhorizon, T *Q_R, T *q_r, T *A_B,\n",
        "               T *c,T* d, cgrps::thread_group g = cgrps::this_thread_block()) {\n",
        "\n",
        "  double* F_lambda = NULL; //can we just use vectors?\n",
        "  double* F_state; //not sure if we need it?\n",
        "  double* F_input; //not sure if we need it - substitute with an array F?\n",
        "  double* Qchol = NULL;\n",
        "  double* Rchol = NULL;\n",
        "  double* zy_temp; // not\n",
        "  double* Q;\n",
        "  double* R;\n",
        "\n",
        "  int k = index;\n",
        "  \n",
        "  //initial timestep\n",
        "  if(index == 0) {\n",
        "        Q = &Q_R[const_step*k]; ///or just Q_R ?\n",
        "        R = &Q_R[const_step*k+nstates*nstates];\n",
        "\n",
        "        //Solve the block system of equations.\n",
        "        glas::copy(nstates*nstates,A_B[k*dyn_step],F_lambda);\n",
        "        glas::scal(nstates*nstates,-1.0, F_lambda); \n",
        "        glas::const(nstates*nstates,F_state, 0);\n",
        "        glas::copy(nstates*ninputs,A_B[k*(nstates*nstates)],F_input);    \n",
        "        glass:chol_InPlace(ninputs, R);\n",
        "        cholSolve_InPlace(R, F.input, ninputs,nstates); //ASK XIAN! Fu = R\\Cu\n",
        "        cholSolve_InPlace(R, q_r[k*(nstates+ninputs)+nstates], ninputs, 1); \n",
        "        //zu = R\\zu\n",
        "\n",
        "        //Solve the block system of eqn overwriting the rhs vector\n",
        "        glas::copy(nstates,d[k*(nstates)],zy_temp); //MatrixCopy(&zy_temp, &z->lambda)\n",
        "        glas::copy(nstates,q_r[k*(nstates+ninputs)], d[k*(nstates)]);\n",
        "        glas::gemv(nstates,nstates,-1.0, Q, zy_temp, -1.0, d[k*(nstates)]);\n",
        "        // zy = - Q * zy - zx\n",
        "        glass:copy(zy_temp,q_r[k*(nstates+ninputs)]);\n",
        "        glass::scal(nstates,-1.0,q_r[k*(nstates+ninputs)]); // zx = -zy\n",
        "        glass:chol_InPlace(nstates,Q) //not sure if that's ok to have it inplace\n",
        "    \n",
        "      } else {\n",
        "          \n",
        "        int level = 0;\n",
        "        Q = &Q_R[const_step*k]; \n",
        "        glass:chol_InPlace(nstates,Q);\n",
        "        \n",
        "        //Not the last timestep\n",
        "        if(k<nhorizon -1) {\n",
        "            //what to do with level? do I just continue to use k?\n",
        "            R = &Q_R[const_step*k+nstaes*nstates];\n",
        "            glass:chol_InPlace(ninputs,R);\n",
        "            cholSolve_InPlace(R, q_r[k*(nstates+ninputs)+nstates],ninputs,1);\n",
        "            //zu = R \\ zu \n",
        "\n",
        "            glass::copy(A_B[k*dyn_step],F_state);\n",
        "            cholSolve_InPlace(Q,F_state,nstates,nstates);\n",
        "\n",
        "            glass::copy(A_B[k*dyn_step+(nstates*nstates)],F_input);\n",
        "            cholSolve_InPlace(R,F_input); \n",
        "        }\n",
        "\n",
        "        //Only the last timestep\n",
        "        cholSolve_InPlace(Q,q_r[k*(nstates+ninputs)]);\n",
        "        //int prev_level = ndlqr_GetIndexLevel(&solver->tree,k-1); ??\n",
        "         // NOTE: This is -I on the state for explicit integration\n",
        "          //For implicit integrators we'd use the A2, B2 partials wrt the next\n",
        "          //state and control\n",
        "        //update k\n",
        "        glas::copy(A_B[k*dyn_step],F_state); // should be -I matrix, chekc manually!\n",
        "        cholSolve_InPlace(Q,F_state);\n",
        "        glas::const(nstates*nstates,F_state, 0.0);\n",
        "        ;\n",
        "        }\n",
        "      \n",
        "  }"
      ],
      "metadata": {
        "id": "8R7pr8PEm8Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Factor Inner Product**"
      ],
      "metadata": {
        "id": "Xp5iQxLJbcbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "//fact_state: Factorization of Q\\A - Do this within function or assume the factorization is passed in as the parameter?\n",
        "//fact_input: Factorization of R\\B\n",
        "//fact_lambda: -A\n",
        "\n",
        "//data_state: A\n",
        "//data_input: B\n",
        "\n",
        "__device__\n",
        "double* FactorInnerProduct(T* A_B, T* fact_state, T* fact_input, T* fact_lambda, int index, int data_level, int fact_level, int nstates, int ninput, int nhorizon) {\n",
        "    //dyn_step: nstates*nstates+ninputs*ninputs;\n",
        "    //k: index + nhorizon*level\n",
        "\n",
        "    double* C1_state; //A, A_B[k * dyn_step] where k is a time step or linear index\n",
        "    double* C1_input; //B, A_B[k * dyn_step+nstates*nstates] where k is a linear index\n",
        "\n",
        "    double* F1_state;\n",
        "    double* F1_input;\n",
        "    double* F1_lambda;\n",
        "\n",
        "    double* C2_state; //A, A_B[k * dyn_step] where k is a time step or linear index\n",
        "    double* C2_input; //B, A_B[k * dyn_step+nstates*nstates] where k is a linear index\n",
        "\n",
        "    double* F2_state;\n",
        "    double* F2_input;\n",
        "    double* F2_lambda;\n",
        "\n",
        "    //GetNdFactor: Retrieve individual NdFactor out of NdData\n",
        "    int linear_index = index + nhorizon * data_level;\n",
        "    C1_state = A_B+(linear_index * dyn_step);\n",
        "    C1_input = A_B+(linear_index * dyn_step + nstates * nstates);\n",
        "\n",
        "    linear_index = index + nhorizon * fact_level; //Not sure\n",
        "    F1_state = fact_state+linear_index;\n",
        "    F1_input = fact_input+linear_index;\n",
        "\n",
        "    linear_index = (index + 1) + nhorizon * data_level;\n",
        "    C2_state = A_B+(linear_index * dyn_step);\n",
        "    C2_input = A_B+(linear_index * dyn_step + nstates * nstates);\n",
        "\n",
        "    linear_index = (index + 1) + nhorizon * fact_level;\n",
        "    F2_state = fact_state+linear_index;\n",
        "    F2_input = fact_input+linear_index;\n",
        "\n",
        "    //Use the precomputed Cholesky factorization to solve for y at each parent level \n",
        "    double *S = F2_lambda;\n",
        "    glass::gemm(nstates, nstates, ninput, 1.0, C1_state, F1_state, -1.0, S); //S = C1x'F1x, why -1.0 and not 0.0?\n",
        "    glass::gemm(nstates, ninput, nstates, 1.0, C1_input, F1_input, 1.0, S);\n",
        "    glass::gemm(nstates, nstates, nstates, 1.0, C2_state, F2_state, 1.0, S);\n",
        "    glass::gemm(nstates, ninput, nstates, C2_input, F2_input, 1.0, S);\n",
        "\n",
        "    return S;\n",
        "}"
      ],
      "metadata": {
        "id": "U_xJ2ajXbiMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SolveCholeskyFactor"
      ],
      "metadata": {
        "id": "31AF0WuH7O8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -n lowerBackSub.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "\n",
        "template <typename T> \n",
        "__device__ \n",
        "void SolveCholeskyFactor(T* fact_state, T* fact_input, T* fact_lambda, int index, int level, int upper_level, int nstates, int ninput, int nhorizon) {\n",
        "    T *s_A = fact_lambda+123; // find what Sbar from level\n",
        "    T *s_b = fact_lambda+123; // find f from upper_level\n",
        "    cholSolve_InPlace(T *s_A, T *s_b, 0, nstates, 1);\n",
        "}"
      ],
      "metadata": {
        "id": "fT4_7isE8pmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Update Shur Factor**"
      ],
      "metadata": {
        "id": "r8yZiSmJcjlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__device__\n",
        "void UpdateShurFactor() {\n",
        "    \n",
        "}"
      ],
      "metadata": {
        "id": "qinh3INWdBaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda diag_Matrix_set.cu\n",
        "\n",
        "#include <cstdint>\n",
        "#include <cooperative_groups.h>\n",
        "namespace cgrps = cooperative_groups;\n",
        "\n",
        "template <typename T>\n",
        "__device__\n",
        "void diag_Matrix_set(T *v_Q, T *m_Q, int n, cgrps::thread_group g) {\n",
        "    \n",
        "    for(uint32_t ind = g.thread_rank(); ind < n; ind+= g.size()){\n",
        "        m_Q[ind*n+ind] = v_Q[n];        \n",
        "    }\n",
        "    \n",
        "}"
      ],
      "metadata": {
        "id": "PgJT0q-_dsJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}